---
title: "Learning and Evaluating Clinical Decision Rules for Cervical Spine Injury"
author: "Jaewon Saw, Jeffrey Cheng, Ahmed Eldeeb, and Kaitlin Smith"
date: 'December, 2022'
header-includes:
   - \usepackage{float}
   - \usepackage{booktabs}
output: 
  pdf_document:
    number_sections: true

---

```{r setup, echo = FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(forcats)
library(lubridate)
library(stringr)
library(RColorBrewer)
library(ggpubr)
library("reshape")
library(formatR)
library(dplyr)
library(glmnet)
library(forcats)
library(flextable)
library(huxtable)
library(glmnet)
library(gridExtra)
library(rpart)
library(rpart.plot)
library(kableExtra)
library(gbm)
library(pROC)

source("./routines.R")

# set default knitr chunks
knitr::opts_chunk$set(
  echo = FALSE,  # don't print the code chunk
  warning = FALSE,  # don't print warnings
  message = FALSE,  # don't print messages
  fig.width = 5,  # set default width of figures
  fig.height = 3,  # set default height of figures
  fig.align = "center",  # always align figure in center
  fig.pos = "H",  # always plot figure at the exact location of the code chunk
  cache = FALSE)  # don't cache results
```

# Introduction

The goal of this project is to create a clinical decision rule to identify 
children who are most likely to have a cervical spine injury (CSI). The adverse 
effects of immobilizing children and subjecting children to ionizing radiation 
motivates such a rule, as there is a desire to minimize the number of children 
unnecessarily subject to radiographic assessment while continuing to maintain 
high sensitivity.

# Data

## Data Collection

The data was taken from the Pediatric Emergency Care Applied Research Network 
(PECARN) public use dataset titled "Predicting Cervical Spine Injury (CSI) in 
Children: A Multi-Centered Case-Control Analysis". A total of 17 PECARN sites 
participated in the study, with a total of 3,314 subjects included in this 
dataset. This dataset was collected between January 2000 and December 2004 and 
was initially procured for the purpose of creating a decision rule for 
identifying factors associated with CSI. The results for this study are 
presented in "Factors Associated With Cervical Spine Injury in Children After
Blunt Trauma" by Leonard et al.

Of the 3,314 records, 540 are deemed positive cervical spine injuries from 
radiology reports or spine consultation. These positive injury records were 
verified by the principal investigator of Leonard et al. and by a pediatric 
<<<<<<< HEAD
neurosurgeon[Lenoard et al]. The remaining 2,774 controls fall into three control 
=======
neurosurgeon. The remaining 2,774 controls fall into three control 
>>>>>>> 00e9311c9587e6f867b345a84ef570445a372ba2
groups: 1,060 unmatched random controls, 1,012 mechanism-of-injury and age 
matched controls, and 702 age-matched EMS controls. 

```{r EDA-Race, echo=FALSE, fig.width=5,fig.height=2}
av <- read.csv("./CSpine/CSV datasets/analysisvariables.csv")
radiology <- read.csv("./CSpine/CSV datasets/radiologyreview.csv")
injury <- read.csv("./CSpine/CSV datasets/injuryclassification.csv")
site <- read.csv("./CSpine/CSV datasets/clinicalpresentationsite.csv")
ems <- read.csv("./CSpine/CSV datasets/clinicalpresentationfield.csv")
outside <- read.csv("./CSpine/CSV datasets/clinicalpresentationoutside.csv")
dem <- read.csv("./CSpine/CSV datasets/demographics.csv")

names(dem)[names(dem) == "studysubjectid"] <- "StudySubjectID"
av_dem <- merge(av, dem, by="StudySubjectID")

plot_demographics_count <- ggplot(data = av_dem[av_dem$Race != "PI" & av_dem$Race != "AI",]) + 
                            geom_count(mapping = aes(x = SITE, y = Race))+
                            ggtitle("Count of Patients by Site and Race")+
                            xlab("Site")+
                            guides(fill=guide_legend(title="Count"))+
                            scale_x_continuous(breaks=seq(1,17,1)) + 
                            theme_bw()
plot_demographics_count
```

In the figure above, it is clear that the patients are not equally distributed 
across PECARN sites, as the racial distribution of patients varies visually 
across sites.

Leonard et al. 6 major variables that are associated with CSI: altered mental status, focal neurological deficit, complaint of neck
pain, substantial injury to the torso, high-risk motor vehicle
crash, and diving. Our analyses focused on these factors, as these were the only 
clinical variables that were provided for all 3,314 records. Each variable is 
not directly comparable with each other, but the variables have already been 
one-hot encoded in the PECARN dataset.

## Cleaning
From the original PECARN public use dataset, the amount of cleaning depended on 
the classifier used. The baseline rule from Leonard et al. and decision trees 
were both tolerant of missing data, so no additional cleaning was done for these 
processes. For the lasso selection of variables and logistic regression, records 
with missing values were removed from the analysis. 

## Training and Evaluation Split
We assume that in practice, our decision rule will be deployed at hospitals not 
included in the dataset. Then, to simulate the performance of the models in 
practice, the data was split into test and training sets based on sites. 

Sites 5, 16, and 17 were randomly chosen as the evaluation sites. 
Leave-one-out-cross-validation (LOOCV) was conducted over the remaining sites 
during training. 

# Modeling

## Replicate Research Findings

First, we implemented the clinical decision rule found in Leonard et al. This
study determined a clinical decision rule by selecting features using forward selection 
with logistic regression. For each iteration of the forward selection process, the algorithm would add the feature whose p-value from 
the Chi-squared statistic was smallest when that feature is added to the model, versus the p-value from when any other
 feature was added. The algorithm stopped adding features when no additional feature 
 resulted in a p-value less than 0.05. 1000 samples were then bootstrapped and a new logistic regression model was computed each time. 
 A covariate was included in the final decision rule if it appeared in over 50% of the bootstrapped models. This process 
was repeated for each control group: random, EMS, and mechanism of injury controls. 
The forward selection logistic regression models identified 6 common covariates between these 3 models: altered mental status, focal neurological deficit, complaint of neck
pain, substantial injury to the torso, high-risk motor vehicle
crash, and diving. The decision rule then classified
a patient as likely to have cervical spine injury if any one of these factors was 
true, otherwise the patient was ruled to not have a cervical spine injury.

Although the report did not specify how missing values 
are handled, we removed samples where any of the covariates were missing, since 
our protocol in R was to remove NA values in logistic regression. 
We calculated this decision rule to have the following metrics, where patients from 
all 3 control groups were included:

```{r, fig.caption = "Baseline metrics calculated from Leonard et al."}
df <- data.frame(c("Sensitivity", "Specificity"), c(0.9064588, 0.4053537), c(0.8795248,  0.3843153), c(0.9333928, 0.4263922))
colnames(df) <- c("Metric", "Estimate", "Lower CI Bound", "Upper CI Bound")
# ft <- flextable(df) %>% 
#   autofit() %>%
#   line_spacing(space = .5, part = "all") %>% 
#   padding(padding = 1, part = "header") 
# ft
knitr::kable(df, "pipe", digits = 3)
```

Although the value for sensitivity is 2% lower and the value for specificity is 5%
higher than presented in the Leonard et al. paper, we were not able to exactly 
replicate results due to assumptions we had to make about handling missing values, 
and which control groups to include in the final calculations. However,
this replication gave us a baseline to compare our own model results to.

## Modeling Approach

In our own modeling efforts we tried several classification methods coupled with two main approaches for features selection. Here we present two of the modeling approaches that we found amenable to interpretability and stability analysis, namely a single decision tree and linear logistic regression. For all of our modeling experiments we employed k-fold Cross Validation, with the folds determined by site (leaving one site out for each fold).
We first outline our feature selection approaches, then present out classification results.

## Feature Selection

### Bootstrapped Forward Selection

First, we selected features using forward selection with logistic regression,
similar to the method used by Leonard et al. We started with an empty model,
and added features sequentially, including the feature with the smallest p-value each iteration.
However, we stopped when there was no feature 
with a p-value less than $0.15$, instead of $0.05$ used in the feature selection method by Leonard et al.
We then proceeded with the 
same bootstrapping procedure, selecting features that appeared in over 50% of the
bootstrapped models. 

### Lasso Logistic Regression (L1 regularization)

Next, we selected features from a Lasso logistic regression model. First,
we completed 10 fold cross validation to find the value of $lambda$ that 
minimizes the $L1$ loss for the training data. We then selected the features from this 
model that had non-zero regression coefficients. 

In the following graph, you can see the order coefficients are added to the 
Lasso model, which provide insights into which features are most important
 to the final probability. 


```{r, fig.caption = "Plot of order features are added to the Lasso model, as the L1 Norm increases. For a large L1 Norm, the lasso solution is equivalent to the OLS solution."}
data_train <- av[av$SITE != 15 & av$SITE != 16 & av$SITE != 7,]
y <- unlist(lapply(data_train$ControlType, function(x){
  if (x == "case")
    {return (1)} 
  else 0}))
data_train$y <- y
vars <- colnames(av)[5:ncol(av)]
dat_no_na <- data_train[rowSums(is.na(data_train)) == 0, ]
X <- dat_no_na[, vars]
lasso_model <- glmnet(x = X, y = dat_no_na$y, alpha = 1, standardize = TRUE)
p1 <- plot(lasso_model,label=TRUE, axes = T)
```


```{r}
ord <- lasso_model$beta
sum_0 <- apply(ord, MARGIN = 1, function(r){
  return(sum(r == 0))
})
# get features in the order they are added to the model
sorted_sum <- data.frame(seq(1,16), names(sort(sum_0)[1:16]))
colnames(sorted_sum) <- c("Order", "Variable")
ft <- flextable(sorted_sum) %>% 
  autofit() %>%
  line_spacing(space = .5, part = "all") %>% 
  padding(padding = 1, part = "header") 
ft
knitr::kable(t(sorted_sum), "pipe", digits = 3)
```


## Classification Experiments

The first classification approach we will present is the single decision tree approach. For that we used the rpart R package [Therneau], and compared the cross validation results with the results from the Leonard et al paper.

```{r, echo=FALSE}
vars <- colnames(av)[5:35]
# convenience subsets (avn excludes the 3 sites, and avnn excludes NAs)
avn <- av[av$SITE != 15 & av$SITE != 16 & av$SITE != 7,]
avn_reprod <-  as.numeric(avn$AlteredMentalStatus == 1 |  avn$FocalNeuroFindings == 1 | 
                            avn$PainNeck == 1 | avn$SubInj_TorsoTrunk == 1 | avn$Predisposed == 1 |
                            avn$HighriskDiving == 1 | avn$HighriskHitByCar == 1 | 
                            avn$HighriskMVC == 1 |   avn$axialloadtop == 1 | avn$Clotheslining == 1)
avn_reprod2 <- as.numeric(avn$AlteredMentalStatus == 1 | avn$FocalNeuroFindings == 1 |  
                            avn$PainNeck == 1| avn$SubInj_TorsoTrunk == 1 | 
                            avn$HighriskDiving == 1 |avn$HighriskMVC == 1)
avnn<- avn %>% drop_na(vars)
avnn_reprod <-  as.numeric(avnn$AlteredMentalStatus == 1 |  avnn$FocalNeuroFindings == 1 | 
                            avnn$PainNeck == 1 | avnn$SubInj_TorsoTrunk == 1 | avnn$Predisposed == 1 |
                            avnn$HighriskDiving == 1 | avnn$HighriskHitByCar == 1 | 
                            avnn$HighriskMVC == 1 |   avnn$axialloadtop == 1 | avnn$Clotheslining == 1)
avnn_reprod2 <- as.numeric(avnn$AlteredMentalStatus == 1 | avnn$FocalNeuroFindings == 1 |  
                            avnn$PainNeck == 1| avnn$SubInj_TorsoTrunk == 1 | 
                            avnn$HighriskDiving == 1 |avnn$HighriskMVC == 1)
#metrics(avnn$ControlType == "case", avnn_reprod)
#metrics(avnn$ControlType == "case", avnn_reprod2)
```

### Single Decision Tree Results

```{r, echo=FALSE}
avnn<- avn %>% drop_na(vars)
num_preds <- rep(0, nrow(avnn))
class_preds <- rep(0, nrow(avnn))
#num_preds <- rep(0, nrow(avn))
#class_preds <- rep(0, nrow(avn))
for (i in unique(avn$SITE)){
  train <-  avn[avn$SITE!=i,]
  test <-  avnn[avnn$SITE==i,]
  
  tree.1 <- rpart(as.numeric(ControlType == "case")~. - SITE - StudySubjectID - CaseID, 
                  data=train, 
                  #weights = (as.numeric(ControlType != "case")+1),
                  method="class",
                  parms = list(prior = c(.32,.68)))
  
  num_preds[avnn$SITE==i] <-  predict(tree.1, newdata=test)[,1]
  class_preds[avnn$SITE==i] <-  as.numeric(predict(tree.1, newdata=test, type="class"))-1
}
r <- rocs(avnn$ControlType == "case", num_preds, class_preds,  avnn_reprod, avnn_reprod2)
#r <- roc(as.numeric(avnn$ControlType == "case"), num_preds, plot=T, lty=1, lwd=2)
#r$auc
```

As we can see, classification tree prodcues an ROC curve with AUC = 0.78. The curve is strictly above the decision rule replicated from the published decision rule. Further selection of an appropriate decision threshold produces the following sensitivity/specificity metrics:

```{r, echo=FALSE}
knitr::kable(metrics(avnn$ControlType == "case", class_preds), "pipe", digits = 3)
```

The final list of predictors used for the decision tree is:

```{r, echo=FALSE}
knitr::kable(tree.1$frame[tree.1$frame$ncompete==4,1], col.names = c("Used Covariates"))
```

And the tree itself can be illustrated as follows:

```{r, echo=FALSE}
rpart.plot(tree.1)
```

One notable advantage to using decisin tree induction, is that most tree induction algorithms can handle missing data, so for the purpose of this classification model we used the whole dataset without having to remove rows with missing data, which should lend the algorithm more statistical power.

### Logistic Regression Results

Next we present the results from simple logistic regression, performed on a set of covariates chosen by the feature selection approaches we outlined above. We combined the sets of covariates selected by forward selection with the set of covariates selected by LASSO and used the intersection of the two sets for higher stability.


```{r, echo=FALSE}
avnn<- avn %>% drop_na(vars)
num_preds <- rep(0, nrow(avnn))
class_preds <- rep(0, nrow(avnn))
#  [1] "AlteredMentalStatus" "FocalNeuroFindings"  "HighriskDiving"      "HighriskHitByCar"
#  [5] "HighriskMVC"         "PainNeck2"           "Predisposed"         "SubInj_TorsoTrunk"
#  [9] "FocalNeuroFindings2" "axialloadtop"        "Torticollis2"        "LOC"
# [13] "HighriskOtherMV"     "Clotheslining"       "SubInj_Head"         "AxialLoadAnyDoc"
#  [1] "AlteredMentalStatus" "FocalNeuroFindings"  "SubInj_Head"         "SubInj_TorsoTrunk"  
#  [5] "Predisposed"         "HighriskDiving"      "HighriskHitByCar"    "HighriskMVC"        
#  [9] "HighriskOtherMV"     "AxialLoadAnyDoc"     "axialloadtop"        "FocalNeuroFindings2"
# [13] "PainNeck2"           "Torticollis2"        "subinj_Head2"        "subinj_Face2"  
for (i in unique(avnn$SITE)){
  train <-  avnn[avnn$SITE!=i,]
  test <-  avnn[avnn$SITE==i,]
  #lm.1 <- glm(as.numeric(ControlType == "case")~ .- SITE - StudySubjectID - CaseID, 
  #          data = train, family = "binomial")
  
  lm.1 <- glm(as.numeric(ControlType == "case")~ AlteredMentalStatus + FocalNeuroFindings + HighriskHitByCar +
                HighriskMVC + PainNeck2 + Predisposed + SubInj_TorsoTrunk + FocalNeuroFindings2 +
                axialloadtop + Torticollis2 + LOC + HighriskOtherMV + Clotheslining + 
                subinj_Face2 + AxialLoadAnyDoc, 
            data = train, family = "binomial")
  num_preds[avnn$SITE==i] <-  predict(lm.1, type = "response", newdata=test)
}
class_preds <- as.numeric(num_preds > 0.072)
r <- rocs(avnn$ControlType == "case", num_preds, class_preds,  avnn_reprod, avnn_reprod2)
# $r$auc
```

Again, the logistic regression produced an ROC curve that was strictly dominant to the published decision rule (according to our CV) with AUC = 0.81. A choice of a suitable decision threshold produced the following sensitivity/specificity metrics:

```{r, echo=FALSE}
knitr::kable(metrics(avnn$ControlType == "case", class_preds), "pipe", digits = 3)
```

The list of predictors used for the logistic regression model is:

```{r, echo=FALSE}
knitr::kable(lm.1$coefficients, col.names =c("Parameter Estimate"), "pipe", digits = 3)
```

In addition to these two approaches we have experimented with neural networks and gradient boosting (on stubs and trees). Both approaches yielded results that were not much better than the approaches we presented here, and produced models that were more difficult to interpret, so we chose to omit them from this report.


## Interpretation

Our model benefits from the interpretability of logistic regression. When 
a patient is inferenced using the model, the model gives a probability of 
that patient having a cervical spine injury. That said, due to the necessity 
for high sensitivity in the model, we declare any patient with a probability
of an injury greater than 7.9% as a patient who needs further imaging. 
It is also possible to view the weights of the features, which demonstrate 
which features contribute the most to the final probability of injury: 

```{r, fig.caption = "Plot of coefficients of the variables in the logistic regression fit to the training data."}
data_train <- av[av$SITE != 15 & av$SITE != 16 & av$SITE != 7,]
# replace these with weights from final model later
lm.1 <- glm(as.numeric(ControlType == "case")~ AlteredMentalStatus + FocalNeuroFindings + HighriskHitByCar +
                HighriskMVC + PainNeck2 + Predisposed + SubInj_TorsoTrunk + FocalNeuroFindings2 +
                axialloadtop + Torticollis2 + LOC + HighriskOtherMV + Clotheslining + 
                subinj_Face2 + AxialLoadAnyDoc, 
            data = data_train, family = "binomial")
weights <- data.frame(names(lm.1$coeff[2:length(lm.1$coeff)]),  lm.1$coeff[2:length(lm.1$coeff)]) # remove intercept
colnames(weights) <- c("Covariate", "Weight")
ggplot(weights, aes(y = Covariate, x = Weight)) + geom_bar(stat="identity", fill = "palegreen3") +
  labs(title = "Weight of Covariates in Logistic Regression") + theme_bw()
```

It is important to note that the values of the coefficients in logistic regression
cannot be interpreted the same way the coefficients in linear regression can be interpreted. 
An increase in any covariate by 1, does not results in linear change in the output by the 
value of $B_j$, where $B_j$ is the value of the coefficient for that variable 
in the model.
Hence, an increase in any of the covariates by one would then yield 
an increase in the odds ratio by $exp(B_j)$, where $B_j$ is the weight 
for that particular covariate. However, coefficients with greater weight 
still can have a greater effect on the output probability from logistic regression,
which we will examine below: 

```{r, fig.caption = "Plot of change of probability for each variable in the when the covariate is 1 vs when the covariate is 0."}
diffs <- rep(0, length(lm.1$coeff[2:length(lm.1$coeff)]))
for (i in 1: length(lm.1$coeff[2:length(lm.1$coeff)])){
  dummy_x <- rep(0, length(lm.1$coeff[2:length(lm.1$coeff)]))
  df_test <- data.frame(matrix(data = dummy_x, nrow = 1))
  colnames(df_test) <- names(lm.1$coeff[2:length(lm.1$coeff)])
  
  p1 <- predict(lm.1, newdata =df_test, type = "response")
  
  dummy_x[i] <- 1
  df_test <- data.frame(matrix(data = dummy_x, nrow = 1))
  colnames(df_test) <- names(lm.1$coeff[2:length(lm.1$coeff)])
  
  p2 <- predict(lm.1, newdata =df_test, type = "response")
  
  diffs[i] <- p2 - p1
}
out_probs <- data.frame(names(lm.1$coeff[2:length(lm.1$coeff)]),  diffs) # remove intercept
colnames(out_probs) <- c("Covariate", "Probability")
ggplot(out_probs, aes(y = Covariate, x = Probability)) + geom_bar(stat="identity", fill = "palegreen3") +
  labs(title = "Effect of Change in Covariate on Outcome", x = "Change in Probability") + theme_bw()
```

In the above plot, we find the difference in the outcome probability of logistic 
regression when the value of each covariate is changed from 0 to 1, while 
all other covariates are held at 0. We chose to hold all other covariates 
at 0 as the mode for each variable is 0. As you can see, the largest coefficients 
from logistic regression have the largest changes in probability when 
that particular covariate is present in the patient. 

However, these particular changes in probability are dependent on all of 
the other covariates being held at 0, so a medical professional could 
not easily state that a patient being predisposed for a cervical spine injury
would have a 10% increase in probability of having a cervical spine injury
versus if they were not predisposed. However, these values do serve to give
a sense on how certain variables affect the outcome more than others.



# Stability

## Model Perturbation

The effect of varying the elasticnet mixing parameter, or the mixture of the 
L1 lasso penalty and L2 ridge penalty, was analyzed. The top 15 variables 
selected by the specific mixing parameter were then used to perform logistic 
regression. To compare the models, the area under the ROC curves (AUC) were 
calculated.

```{r glmnet-alpha, echo = FALSE, include=TRUE, message = FALSE, warning= FALSE, out.width = '80%', fig.align='center'}

av <- read.csv("./CSpine/CSV datasets/analysisvariables.csv")

vars <- colnames(av)[5:ncol(av)]

# remove testing data
data_train <- av %>% filter(SITE != 5 & SITE != 16 & SITE != 15)

y <- unlist(lapply(data_train$ControlType, function(x){
  if (x == "case")
    {return (1)} 
  else 0}))

data_train$y <- y


dat_no_na <- data_train[rowSums(is.na(data_train)) == 0, ]


# Setting alpha = 1 implements lasso regression
X <- dat_no_na[, vars]

# get baseline deicsion rule results 
avnn_reprod <-  as.numeric(dat_no_na$AlteredMentalStatus == 1 |  dat_no_na$FocalNeuroFindings == 1 | 
                            dat_no_na$PainNeck == 1 | dat_no_na$SubInj_TorsoTrunk == 1 | dat_no_na$Predisposed == 1 |
                            dat_no_na$HighriskDiving == 1 | dat_no_na$HighriskHitByCar == 1 | 
                            dat_no_na$HighriskMVC == 1 |   dat_no_na$axialloadtop == 1 | dat_no_na$Clotheslining == 1)

avnn_reprod2 <- as.numeric(dat_no_na$AlteredMentalStatus == 1 | dat_no_na$FocalNeuroFindings == 1 |  
                            dat_no_na$PainNeck == 1| dat_no_na$SubInj_TorsoTrunk == 1 | 
                            dat_no_na$HighriskDiving == 1 |dat_no_na$HighriskMVC == 1)

alpha_range = seq(1,0,-0.1)
sens_low = c()
sens_high = c()
sens = c()

spec_low = c()
spec_high = c()
spec = c()

auc_list = c()
for (j in alpha_range){
  X <- dat_no_na[, vars]
  # find value of lambda that minimizes the deviance
  lasso_reg <- glmnet::cv.glmnet(as.matrix(X),  y = dat_no_na$y, alpha = j, standardize = TRUE, nfolds = 14)
  
  lambda_best <- lasso_reg$lambda.min
  
  
  # get lasso model with optimal performance found with cross validation
  
  lasso_model <- glmnet(x = X, y = dat_no_na$y, alpha = j, standardize = TRUE)
  
  ord <- lasso_model$beta
  
  sum_0 <- apply(ord, MARGIN = 1, function(r){
    return(sum(r == 0))
  })
  
  # get features in the order they are added to the model
  sorted_sum <- data.frame(sort(sum_0))
  
  selected_vars <- rownames(sorted_sum)[1:16]
  
  # get cross validation results for the different sites
  
  num_preds <- rep(0, nrow(dat_no_na))
  class_preds <- rep(0, nrow(dat_no_na))
  
  for (i in unique(dat_no_na$SITE)){
    train <-  dat_no_na[dat_no_na$SITE!=i,]
    test <-  dat_no_na[dat_no_na$SITE==i,]
    
    X <- train[, selected_vars]
    X_test <- test[, selected_vars]
    
    lasso_model <- glmnet(x = X, y = train$y, alpha = j, lambda = lambda_best, standardize = TRUE)
    
    num_preds[dat_no_na$SITE==i] <-  predict(lasso_model, type = "response", newx = as.matrix(X_test))
    
  }
  
  class_preds <- as.numeric(num_preds > 0.085)
  
  metrics(dat_no_na$ControlType == "case", class_preds)



  class_preds <- as.numeric(num_preds > 0.085)
  results <- metrics(dat_no_na$ControlType == "case", class_preds)

  sens = c(sens, results[1,1])
  sens_low = c(sens_low, results[1,2])
  sens_high = c(sens_high, results[1,3])

  spec = c(spec, results[2,1])
  spec_low = c(spec_low, results[2,2])
  spec_high = c(spec_high, results[2,3])
  
  # rocs(dat_no_na$ControlType == "case", num_preds, class_preds,  avnn_reprod, avnn_reprod2)
  auc_list <- c(auc_list, auc(roc(as.numeric(dat_no_na$ControlType == "case"), num_preds, plot=F, lty=1, lwd=2)))
  
  
}
alpha_auc_df <- data.frame(x = alpha_range,
                                     y = auc_list)

ggplot(alpha_auc_df, aes(x, y)) + geom_point() + 
  xlab('Lasso-Ridge Mixing Parameter') + 
ylab('AUC') + ggtitle('Area under ROC Curve Across Varying Mixing Parameters') +
  theme_bw()
# CI_bootstrap_sens <-round(data.frame(x = alpha_range,
#                                      sens = sens,
#                                      sens_low = sens_low,
#                                      sens_high = sens_high), 4)
#   
# # Creating scatter plot with its
# # confindence intervals
# ggplot(CI_bootstrap_sens, aes(x, sens)) + geom_point() + 
# geom_errorbar(aes(ymin = sens_low, ymax = sens_high))+ xlab('Lasso-Ridge Mixing Parameter') + 
# ylab('Sensitivity Estimate') + ggtitle('Area under ROC Curve Across Varying Mixing Parameters')
# 
# CI_bootstrap_spec <-round(data.frame(x = alpha_range,
#                                      spec = spec,
#                                      spec_low = spec_low,
#                                      spec_high = spec_high), 4)
#   
# # Creating scatter plot with its
# # confindence intervals
# ggplot(CI_bootstrap_spec, aes(x, spec)) + geom_point() + 
# geom_errorbar(aes(ymin = spec_low, ymax = spec_high)) + xlab('Lasso-Ridge Mixing Parameter') +
# ylab('Specificity Estimate') + ggtitle('Specificity Estimates Across Varying Mixing Parameters')

```
The nonzero $\alpha$ parameters yield similar AUC values, but the pure ridge 
regression $\alpha$ yields significantly worse performance.

As for introducing a perturbation to logistic regression classification, one can 
adjust the classification threshold. The effect of this is captured in the ROC 
curve of the model presented in the previous section.


## Data Perturbation

```{r Base-model}
avn <- av[av$SITE != 15 & av$SITE != 16 & av$SITE != 7,]
vars <- colnames(av)[5:26]
avnn<- avn %>% drop_na(vars)

num_preds <- rep(0, nrow(avnn))
class_preds <- rep(0, nrow(avnn))


#  [1] "AlteredMentalStatus" "FocalNeuroFindings"  "HighriskDiving"      "HighriskHitByCar"
#  [5] "HighriskMVC"         "PainNeck2"           "Predisposed"         "SubInj_TorsoTrunk"
#  [9] "FocalNeuroFindings2" "axialloadtop"        "Torticollis2"        "LOC"
# [13] "HighriskOtherMV"     "Clotheslining"       "SubInj_Head"         "AxialLoadAnyDoc"


#  [1] "AlteredMentalStatus" "FocalNeuroFindings"  "SubInj_Head"         "SubInj_TorsoTrunk"  
#  [5] "Predisposed"         "HighriskDiving"      "HighriskHitByCar"    "HighriskMVC"        
#  [9] "HighriskOtherMV"     "AxialLoadAnyDoc"     "axialloadtop"        "FocalNeuroFindings2"
# [13] "PainNeck2"           "Torticollis2"        "subinj_Head2"        "subinj_Face2"  


for (i in unique(avnn$SITE)){
  train <-  avnn[avnn$SITE!=i,]
  test <-  avnn[avnn$SITE==i,]

  #lm.1 <- glm(as.numeric(ControlType == "case")~ .- SITE - StudySubjectID - CaseID, 
  #          data = train, family = "binomial")
  
  lm.1 <- glm(as.numeric(ControlType == "case")~ AlteredMentalStatus + FocalNeuroFindings + HighriskHitByCar +
                HighriskMVC + PainNeck2 + Predisposed + SubInj_TorsoTrunk + FocalNeuroFindings2 +
                axialloadtop + Torticollis2 + LOC + HighriskOtherMV + Clotheslining + 
                subinj_Face2 + AxialLoadAnyDoc, 
            data = train, family = "binomial")
  num_preds[avnn$SITE==i] <-  predict(lm.1, type = "response", newdata=test)
}
class_preds <- as.numeric(num_preds > 0.079)

base_metrics <- metrics(avnn$ControlType == "case", class_preds)
```


### Change in Covariate Distribution During Testing

To examine the effect of changing the covariate distribution of the test set, 
the model was evaluated on the held-out site 7. As seen in Figure 1, site 7 has 
a higher proportion of of subjects of race group what was not disclosed (ND).

```{r Covariate}
# data from site 7
av7 <- av[av$SITE == 7,]
vars <- colnames(av)[5:26]
av7n<- av7 %>% drop_na(vars)

# apply base model
num_preds <- rep(0, nrow(av7n))
class_preds <- rep(0, nrow(av7n))

num_preds <-  predict(lm.1, type = "response", newdata=av7n)


class_preds <- as.numeric(num_preds > 0.079)
# rocs(av7n$ControlType == "case", num_preds, class_preds,  avnn_reprod, avnn_reprod2)
site7_metrics <- metrics(av7n$ControlType == "case", class_preds)

df <- data.frame(c(base_metrics[1,1], site7_metrics[1,1]), 
                 c(base_metrics[1,2], site7_metrics[1,2]), 
                 c(base_metrics[1,3], site7_metrics[1,3]))
colnames(df) <- c("Estimate", "Lower CI Bound", "Upper CI Bound")
rownames(df) <- c("Orig. Training Data", "Site 7 Data")
df <- df %>% rownames_to_column(" ")
ft <- flextable(df) %>% 
  autofit() %>%
  line_spacing(space = .5, part = "all") %>% 
  padding(padding = 1, part = "header") 
ft <- flextable::set_caption(ft, "Sensitivity Confidence Intervals")
ft
```
As expected, worse performance is observed, as the records from site 7 were 
excluded from training. 

### Stability under Subsampling
The model was evaluated on just the positive injury records and the EMS control 
group.

```{r Subsampling}
# case and ems data
avn_ce <- av[av$ControlType == 'case' | av$ControlType == 'ems',]
vars <- colnames(av)[5:26]
avnn_ce<- avn_ce %>% drop_na(vars)

# apply base model
num_preds <- rep(0, nrow(avnn_ce))
class_preds <- rep(0, nrow(avnn_ce))

num_preds <-  predict(lm.1, type = "response", newdata=avnn_ce)


class_preds <- as.numeric(num_preds > 0.079)
# rocs(avnn_ce$ControlType == "case", num_preds, class_preds,  avnn_reprod, avnn_reprod2)
subsample_metrics <- metrics(avn_ce$ControlType == "case", class_preds)
df <- data.frame(c(base_metrics[1,1], subsample_metrics[1,1]), 
                 c(base_metrics[1,2], subsample_metrics[1,2]), 
                 c(base_metrics[1,3], subsample_metrics[1,3]))
colnames(df) <- c("Estimate", "Lower CI Bound", "Upper CI Bound")
rownames(df) <- c("Orig. Training Data", "Case and EMS Data")
df <- df %>% rownames_to_column(" ")
ft <- flextable(df) %>% 
  autofit() %>%
  line_spacing(space = .5, part = "all") %>% 
  padding(padding = 1, part = "header") 
ft <- flextable::set_caption(ft, "Sensitivity Confidence Intervals")
ft
```



### Stability under Bootstrapping

In addition, we observed the stability our model under bootstraps. For the lasso 
selection of variables, we analyze the frequency of the original 15 factors in 
our model in the top 15 factors selected by lasso generated by the bootstrapped 
data.


```{r Bootstrap-Lasso}
source("./routines.R")
library(glmnet)
library(ggplot2)

av <- read.csv("./CSpine/CSV datasets/analysisvariables.csv")

vars <- colnames(av)[5:ncol(av)]

# remove testing data
data_train <- av %>% filter(SITE != 7 & SITE != 16 & SITE != 15)

y <- unlist(lapply(data_train$ControlType, function(x){
  if (x == "case")
    {return (1)} 
  else 0}))

data_train$y <- y


dat_no_na <- data_train[rowSums(is.na(data_train)) == 0, ]

# get baseline deicsion rule results 
avnn_reprod <-  as.numeric(dat_no_na$AlteredMentalStatus == 1 |  dat_no_na$FocalNeuroFindings == 1 | 
                            dat_no_na$PainNeck == 1 | dat_no_na$SubInj_TorsoTrunk == 1 | dat_no_na$Predisposed == 1 |
                            dat_no_na$HighriskDiving == 1 | dat_no_na$HighriskHitByCar == 1 | 
                            dat_no_na$HighriskMVC == 1 |   dat_no_na$axialloadtop == 1 | dat_no_na$Clotheslining == 1)

avnn_reprod2 <- as.numeric(dat_no_na$AlteredMentalStatus == 1 | dat_no_na$FocalNeuroFindings == 1 |  
                            dat_no_na$PainNeck == 1| dat_no_na$SubInj_TorsoTrunk == 1 | 
                            dat_no_na$HighriskDiving == 1 |dat_no_na$HighriskMVC == 1)

# Setting alpha = 1 implements lasso regression
X <- dat_no_na[, vars]

# find value of lambda that minimizes the deviance
lasso_reg <- glmnet::cv.glmnet(as.matrix(X),  y = dat_no_na$y, alpha = 1, standardize = TRUE, nfolds = 14)

lambda_best <- lasso_reg$lambda.min

# get lasso model with optimal performance found with cross validation

lasso_model <- glmnet(x = X, y = dat_no_na$y, alpha = 1, standardize = TRUE)

ord <- lasso_model$beta

sum_0 <- apply(ord, MARGIN = 1, function(r){
  return(sum(r == 0))
})

# get features in the order they are added to the model
sorted_sum <- data.frame(sort(sum_0))

selected_names <- rownames(sorted_sum)[1:15]



selected_names_counts = integer(length(selected_names))

sens_low = c()
sens_high = c()
sens = c()

spec_low = c()
spec_high = c()
spec = c()

# do n_bootstraps bootstraps (sample with replacement)
set.seed(1)
n_bootstraps = 100
for (i in 1:n_bootstraps){
  # resample from data_train
  data_train_boot <- data_train[sample(nrow(data_train),size=nrow(data_train),replace=TRUE),]
  y <- unlist(lapply(data_train_boot$ControlType, function(x){
  if (x == "case")
    {return (1)} 
  else 0}))

  data_train_boot$y <- y
  
  
  dat_no_na <- data_train_boot[rowSums(is.na(data_train_boot)) == 0, ]
  X <- dat_no_na[, vars]

  # find value of lambda that minimizes the deviance
  lasso_reg <- glmnet::cv.glmnet(as.matrix(X),  y = dat_no_na$y, alpha = 1, standardize = TRUE, nfolds = 14)
  
  lambda_best <- lasso_reg$lambda.min
  
  # get lasso model with optimal performance found with cross validation
  
  lasso_model <- glmnet(x = X, y = dat_no_na$y, alpha = 1, standardize = TRUE)
  
  ord <- lasso_model$beta
  
  sum_0 <- apply(ord, MARGIN = 1, function(r){
    return(sum(r == 0))
  })
  
  # get features in the order they are added to the model
  sorted_sum <- data.frame(sort(sum_0))
  
  bootstrapped_names <- rownames(sorted_sum)[1:16]
   
  for (j in 1:length(selected_names)){
    if (selected_names[j] %in% bootstrapped_names){
      selected_names_counts[j] <- selected_names_counts[j] + 1
    }
  }
  selected_names_freq <- selected_names_counts/n_bootstraps
  
  
  # num_preds <- rep(0, nrow(dat_no_na))
  # class_preds <- rep(0, nrow(dat_no_na))
  # 
  # for (i in unique(dat_no_na$SITE)){
  #   train <-  dat_no_na[dat_no_na$SITE!=i,]
  #   test <-  dat_no_na[dat_no_na$SITE==i,]
  #   
  #   X <- train[, vars]
  #   X_test <- test[, vars]
  #   
  #   lasso_model <- glmnet(x = X, y = train$y, alpha = 1, lambda = lambda_best, standardize = TRUE)
  #   
  #   num_preds[dat_no_na$SITE==i] <-  predict(lasso_model, type = "response", newx = as.matrix(X_test))
  #   
  # }
  # 
  # 
  # class_preds <- as.numeric(num_preds > 0.085)
  # results <- metrics(dat_no_na$ControlType == "case", class_preds)
  # 
  # sens = c(sens, results[1,1])
  # sens_low = c(sens_low, results[1,2])
  # sens_high = c(sens_high, results[1,3])
  # 
  # spec = c(spec, results[2,1])
  # spec_low = c(spec_low, results[2,2])
  # spec_high = c(spec_high, results[2,3])
  

  
}
names(selected_names_freq) <- selected_names

df_freq <- data.frame(selected_names, selected_names_freq)
colnames(df_freq) <- c("Feature", "Frequency")

ggplot(df_freq, aes(y = Feature, x = Frequency)) + geom_bar(stat= "identity", fill = "palegreen3") + 
  labs(title = "Frequency of Selected Features During Bootstrapping") + theme_bw()

```

We can see that the top of the original factors appear in the bootstrapped 
data's top factors very frequently, whereas the lower of the original factors 
appear less frequently.

The stability of the logistic regression classifier under bootstrapping was also 
analyzed. The sensitivity and specificity confidence intervals of each bootstrap 
were recorded and are plotted below.
```{r Bootstrap-Logistic}
for (j in 1:n_bootstraps){
  avn_boot <- avn[sample(nrow(avn),size=nrow(avn),replace=TRUE),]
  avnn <- avn_boot %>% drop_na(vars)
  
  num_preds <- rep(0, nrow(avnn))
  class_preds <- rep(0, nrow(avnn))
  
  
  #  [1] "AlteredMentalStatus" "FocalNeuroFindings"  "HighriskDiving"      "HighriskHitByCar"
  #  [5] "HighriskMVC"         "PainNeck2"           "Predisposed"         "SubInj_TorsoTrunk"
  #  [9] "FocalNeuroFindings2" "axialloadtop"        "Torticollis2"        "LOC"
  # [13] "HighriskOtherMV"     "Clotheslining"       "SubInj_Head"         "AxialLoadAnyDoc"
  
  
  #  [1] "AlteredMentalStatus" "FocalNeuroFindings"  "SubInj_Head"         "SubInj_TorsoTrunk"  
  #  [5] "Predisposed"         "HighriskDiving"      "HighriskHitByCar"    "HighriskMVC"        
  #  [9] "HighriskOtherMV"     "AxialLoadAnyDoc"     "axialloadtop"        "FocalNeuroFindings2"
  # [13] "PainNeck2"           "Torticollis2"        "subinj_Head2"        "subinj_Face2"  
  
  
  for (i in unique(avnn$SITE)){
    train <-  avnn[avnn$SITE!=i,]
    test <-  avnn[avnn$SITE==i,]
  
    #lm.1 <- glm(as.numeric(ControlType == "case")~ .- SITE - StudySubjectID - CaseID, 
    #          data = train, family = "binomial")
    
    lm.1 <- glm(as.numeric(ControlType == "case")~ AlteredMentalStatus + FocalNeuroFindings + HighriskHitByCar +
                  HighriskMVC + PainNeck2 + Predisposed + SubInj_TorsoTrunk + FocalNeuroFindings2 +
                  axialloadtop + Torticollis2 + LOC + HighriskOtherMV + Clotheslining + 
                  subinj_Face2 + AxialLoadAnyDoc, 
              data = train, family = "binomial")
  
    num_preds[avnn$SITE==i] <-  predict(lm.1, type = "response", newdata=test)
  }
  
  class_preds <- as.numeric(num_preds > 0.079)
  
  results <- metrics(avnn$ControlType == "case", class_preds)

  sens = c(sens, results[1,1])
  sens_low = c(sens_low, results[1,2])
  sens_high = c(sens_high, results[1,3])

  spec = c(spec, results[2,1])
  spec_low = c(spec_low, results[2,2])
  spec_high = c(spec_high, results[2,3])
}

sens_sort <- sort(sens, decreasing=T)
sens_low_sort <- sens_low[order(sens, decreasing=T)]
sens_high_sort <- sens_high[order(sens, decreasing=T)]

spec_sort <- sort(spec, decreasing=T)
spec_low_sort <- spec_low[order(spec, decreasing=T)]
spec_high_sort <- spec_high[order(spec, decreasing=T)]


CI_bootstrap_sens <-round(data.frame(x = 1:n_bootstraps,
                                     sens = sens_sort,
                                     sens_low = sens_low_sort,
                                     sens_high = sens_high_sort), 4)
  
# Creating scatter plot with its
# confindence intervals
ggplot(CI_bootstrap_sens, aes(x, sens)) + geom_point() + 
geom_errorbar(aes(ymin = sens_low, ymax = sens_high))+ xlab('Bootstraps (Sorted by Sensitivity Est.)') + 
ylab('Sensitivity Estimate') + ggtitle('Sensitivity Estimates Across Bootstraps')

CI_bootstrap_spec <-round(data.frame(x = 1:n_bootstraps,
                                     spec = spec_sort,
                                     spec_low = spec_low_sort,
                                     spec_high = spec_high_sort), 4)
  
# Creating scatter plot with its
# confindence intervals
ggplot(CI_bootstrap_spec, aes(x, spec)) + geom_point() + 
<<<<<<< HEAD
geom_errorbar(aes(ymin = spec_low, ymax = spec_high)) + xlab('Sample') +
ylab('Specificity Estimate') + ggtitle('Specificity Estimates Across Bootstraps') + theme_bw()
=======
geom_errorbar(aes(ymin = spec_low, ymax = spec_high)) + xlab('Bootstraps (Sorted by Specificity Est.)') +
ylab('Specificity Estimate') + ggtitle('Specificity Estimates Across Bootstraps')
>>>>>>> 00e9311c9587e6f867b345a84ef570445a372ba2

```
The sensitivity estimates appear to be more stable than the specificity 
estimates, as the sensitivity confidence intervals share more overlap.


# Evaluation

## Three Test Sites
As stated in the subsection _Training and Evaluation Split_, three hospitals were randomly chosen as the sites for evaluation: 5, 16, and 17. These three sites were left out during the exploratory data analysis, model training, and model stability assessment.

## Baseline Performance
We first applied the decision rule derived by Leonard et al. (2021) to the three test sites to evaluate the baseline performance, to which the relative performance of our models would later be evaluated. Using the 6-variable decision rule (altered mental status, focal neurologic deficit, complaint of neck pain, substantial injury to the torso, high-risk motor vehicle crash, and diving), the sensitivity and specificity of identifying cervical spine injury by the presence of at least one of these six factors were 85% (95% CI 78% to 92%) and 44% (95% CI 39% and 49%), respectively. 
```{r sensitivity decision rule from paper, echo = FALSE, include=FALSE, message = FALSE, warning= FALSE}
av <- read.csv("./CSpine/CSV datasets/analysisvariables.csv")
# THE THREE EVALUATION SITES ONLY
av <- av[av$SITE== 15 | av$SITE == 16 | av$SITE == 7,]

# get all potential factors in the analysisvariables.csv: AlteredMentalStatus - subInj_TorsoTrunk2
# variable names only
vars <- colnames(av)[5:26]

# drop ALL rows with *any* NA values
# filter and get "case" only
av.sense <- av %>% drop_na(vars) %>% filter(ControlType == "case")

tp <- av.sense %>% filter(AlteredMentalStatus == 1 | FocalNeuroFindings == 1 |  PainNeck == 1| SubInj_TorsoTrunk == 1 | 
HighriskDiving == 1 |HighriskMVC == 1)
 

sensitivity <- DescTools::BinomCI(nrow(tp), n = nrow(av.sense), method = "wald", conf.level = 0.95)

# as specified in the paper, we drop any rows where any of the indicators are missing 
# filter and get NON-case only
av.spec <- av %>% drop_na(vars) %>% filter(ControlType != "case")

tn <- av.spec %>% filter(AlteredMentalStatus == 0 & FocalNeuroFindings == 0 & PainNeck == 0 & SubInj_TorsoTrunk == 0 & HighriskDiving == 0 & HighriskMVC == 0)

specificity <- DescTools::BinomCI(nrow(tn), n = nrow(av.spec), method = "wald", conf.level = 0.95)


df <- rbind(sensitivity, specificity)
colnames(df) <- c("Estimate", "Lower CI Bound", "Upper CI Bound")
rownames(df) <- c("Sensitivity", "Specificity")
df
```

```{r, echo = FALSE, include=TRUE, message = FALSE, warning= FALSE, out.width = '80%', fig.align='center'}
kable(round(df,2), align = 'cccc',
      caption = 'Decision Rule from Leonard et al. (2011)') %>%
  kable_styling(bootstrap_options = "striped", full_width = F, latex_options = "HOLD_position")
```

## Forward Variable Selection Performance
We then applied the decision rule derived by using forward variable selection on the decision rule by Leonard et al. (2011) to the three test sites. This decision rule consisted of four additional variables: conditions predisposing to cervical spine injury, high-risk hit by car, axial load to top of the head, and injury by clothes-lining. (Note that at each forward step, the forward selection process adds the one variable that gives the single best improvement to the model. Hence, it is possible for the final model to contain variables that are significant when added at each respective step but are no longer as significant in the presence of subsequently added variables (Leonard et al. 2021).) The sensitivity and specificity of identifying cervical spine injury by the presence of at least one of these ten factors was 88% (95% CI 82% to 95%) and 30% (95% CI 26% and 35%), respectively. Compared to the baseline, there was a slight increase in the sensitivity (3% increase) and decrease in the specificity (14% decrease). 

```{r sensitivity from forward selection, echo = FALSE, include=FALSE, message = FALSE, warning= FALSE}
av.sense <- av %>% drop_na(vars) %>% filter(ControlType == "case")

tp <- av.sense %>% filter(AlteredMentalStatus == 1 |  FocalNeuroFindings == 1 | PainNeck == 1 |  SubInj_TorsoTrunk == 1 | Predisposed == 1 | HighriskDiving == 1 | HighriskHitByCar == 1 | HighriskMVC == 1 |        
 axialloadtop == 1 | Clotheslining == 1)
 

sensitivity <- DescTools::BinomCI(nrow(tp), n = nrow(av.sense), method = "wald", conf.level = 0.95)

# as specified in the paper, we drop any rows where any of the 
# indicators are missing 
av.spec <- av %>% drop_na(vars) %>% filter(ControlType != "case")

tn <- av.spec %>% filter(AlteredMentalStatus == 0 &  FocalNeuroFindings == 0 & PainNeck == 0 & SubInj_TorsoTrunk == 0 & Predisposed == 0 & HighriskDiving == 0 & HighriskHitByCar == 0 & HighriskMVC == 0 &        
 axialloadtop == 0 & Clotheslining == 0)

specificity <- DescTools::BinomCI(nrow(tn), n = nrow(av.spec), method = "wald", conf.level = 0.95)


df <- rbind(sensitivity, specificity)
colnames(df) <- c("Estimate", "Lower CI Bound", "Upper CI Bound")
rownames(df) <- c("Sensitivity", "Specificity")
df
```

```{r, echo = FALSE, include=TRUE, message = FALSE, warning= FALSE, out.width = '80%', fig.align='center'}
kable(round(df,2), align = 'cccc',
      caption = 'Forward Variable Selection') %>%
  kable_styling(bootstrap_options = "striped", full_width = F, latex_options = "HOLD_position")
```

# Single Decision Tree Performance
We tested the single decision tree model (using the aforementioned ten variables) to the three test sites. The sensitivity and specificity was 91% (95% CI 85% to 96%) and 47% (95% CI 42% and 52%), respectively. Compared to the baseline, there were increases in both the sensitivity (6% increase) and in the specificity (3% increase).

```{r, echo = FALSE, include=FALSE, message = FALSE, warning= FALSE}

# convenience subsets (avn excludes the 3 sites, and avnn excludes NAs)

avn <- av[av$SITE== 15 | av$SITE == 16 | av$SITE == 7,]

avn_reprod <-  as.numeric(avn$AlteredMentalStatus == 1 |  avn$FocalNeuroFindings == 1 | 
                            avn$PainNeck == 1 | avn$SubInj_TorsoTrunk == 1 | avn$Predisposed == 1 |
                            avn$HighriskDiving == 1 | avn$HighriskHitByCar == 1 | 
                            avn$HighriskMVC == 1 |   avn$axialloadtop == 1 | avn$Clotheslining == 1)

avn_reprod2 <- as.numeric(avn$AlteredMentalStatus == 1 | avn$FocalNeuroFindings == 1 |  
                            avn$PainNeck == 1| avn$SubInj_TorsoTrunk == 1 | 
                            avn$HighriskDiving == 1 |avn$HighriskMVC == 1)


avnn<- avn %>% drop_na(vars)

avnn_reprod <-  as.numeric(avnn$AlteredMentalStatus == 1 |  avnn$FocalNeuroFindings == 1 | 
                            avnn$PainNeck == 1 | avnn$SubInj_TorsoTrunk == 1 | avnn$Predisposed == 1 |
                            avnn$HighriskDiving == 1 | avnn$HighriskHitByCar == 1 | 
                            avnn$HighriskMVC == 1 |   avnn$axialloadtop == 1 | avnn$Clotheslining == 1)

avnn_reprod2 <- as.numeric(avnn$AlteredMentalStatus == 1 | avnn$FocalNeuroFindings == 1 |  
                            avnn$PainNeck == 1| avnn$SubInj_TorsoTrunk == 1 | 
                            avnn$HighriskDiving == 1 |avnn$HighriskMVC == 1)


metrics(avnn$ControlType == "case", avnn_reprod)
metrics(avnn$ControlType == "case", avnn_reprod2)
```

```{r, echo = FALSE, include=TRUE, message = FALSE, warning= FALSE, out.width = '80%', fig.align='center'}

avnn<- avn %>% drop_na(vars)
num_preds <- rep(0, nrow(avnn))
class_preds_tree <- rep(0, nrow(avnn))
#num_preds <- rep(0, nrow(avn))
#class_preds <- rep(0, nrow(avn))
for (i in unique(avn$SITE)){
  train <-  avn[avn$SITE!=i,]
  test <-  avnn[avnn$SITE==i,]
  
  tree.1 <- rpart(as.numeric(ControlType == "case")~. - SITE - StudySubjectID - CaseID, 
                  data=train, 
                  #weights = (as.numeric(ControlType != "case")+1),
                  method="class",
                  parms = list(prior = c(.32,.68)))
  
  num_preds[avnn$SITE==i] <-  predict(tree.1, newdata=test)[,1]
  class_preds_tree[avnn$SITE==i] <-  as.numeric(predict(tree.1, newdata=test, type="class"))-1
}
r <- rocs(avnn$ControlType == "case", num_preds, class_preds,  avnn_reprod, avnn_reprod2)
single_tree_metrics <- metrics(avnn$ControlType == "case", class_preds_tree)
```

```{r, echo = FALSE, include=TRUE, message = FALSE, warning= FALSE, out.width = '80%', fig.align='center'}
kable(round(single_tree_metrics,2), align = 'cccc',
      caption = 'Single Decision Tree with 10 Variables') %>%
kable_styling(bootstrap_options = "striped", full_width = F, latex_options = "HOLD_position")
```

# Gradient-Boosted Tree Performance
We tested the gradient-boosted decision tree model to the three test sites. The sensitivity and specificity was 92% (95% CI 86% to 97%) and 41% (95% CI 36% and 46%), respectively. Compared to the single-tree model, there was a slight increase in the sensitivity (1% increase) and decrease in the specificity (6% decrease). Compared to the baseline, there was an increase in the sensitivity (7% increase) and a decrease in the specificity (3% decrease).
```{r gradient boosted trees or stubs, echo = FALSE, include=TRUE, message = FALSE, warning= FALSE, out.width = '80%', fig.align='center'}

avnn<- avn %>% drop_na(vars)

num_preds <- rep(0, nrow(avnn))
class_preds <- rep(0, nrow(avnn))

for (i in unique(avn$SITE)){
  train <-  avn[avn$SITE!=i,]
  test <-  avnn[avnn$SITE==i,]
  
  gbm.1 <- gbm(as.numeric(ControlType == "case")~. - SITE - StudySubjectID - CaseID, data=train,
             distribution = "bernoulli",
              n.trees = 90,
              interaction.depth = 2,
              n.minobsinnode = 50,
              shrinkage = 0.1,
              bag.fraction = 0.7,
              train.fraction = 0.9,
              cv.folds = 7,
              verbose = F,
              n.cores = 5)

  num_preds[avnn$SITE==i] <-  predict(gbm.1, type = "response", newdata=test)
}

class_preds <- as.numeric(num_preds > 0.087)

rocs(avnn$ControlType == "case", num_preds, class_preds,  avnn_reprod, avnn_reprod2)
gb_tree_metrics <- metrics(avnn$ControlType == "case", class_preds)

```

```{r, echo = FALSE, include=TRUE, message = FALSE, warning= FALSE, out.width = '80%', fig.align='center'}
kable(round(gb_tree_metrics,2), align = 'cccc',
      caption = 'Gradient-Boosted Tree') %>%
kable_styling(bootstrap_options = "striped", full_width = F, latex_options = "HOLD_position")
```

# Logistic Regression
We tested the logistic regression model (based on all variables from feature selection) to the three test sites. The sensitivity and specificity was 88% (95% CI 82% to 95%) and 43% (95% CI 38% and 48%), respectively. Compared to the baseline, there were increases in both the sensitivity (3% increase) and in the specificity (1% increase).

```{r logistic regression (2), echo = FALSE, include=TRUE, message = FALSE, warning= FALSE, out.width = '80%', fig.align='center'}
# CHANGE FACTORS 

avnn<- avn %>% drop_na(vars)
num_preds <- rep(0, nrow(avnn))
class_preds <- rep(0, nrow(avnn))
#  [1] "AlteredMentalStatus" "FocalNeuroFindings"  "HighriskDiving"      "HighriskHitByCar"
#  [5] "HighriskMVC"         "PainNeck2"           "Predisposed"         "SubInj_TorsoTrunk"
#  [9] "FocalNeuroFindings2" "axialloadtop"        "Torticollis2"        "LOC"
# [13] "HighriskOtherMV"     "Clotheslining"       "SubInj_Head"         "AxialLoadAnyDoc"

#  [1] "AlteredMentalStatus" "FocalNeuroFindings"  "SubInj_Head"         "SubInj_TorsoTrunk"  
#  [5] "Predisposed"         "HighriskDiving"      "HighriskHitByCar"    "HighriskMVC"        
#  [9] "HighriskOtherMV"     "AxialLoadAnyDoc"     "axialloadtop"        "FocalNeuroFindings2"
# [13] "PainNeck2"           "Torticollis2"        "subinj_Head2"        "subinj_Face2"  
for (i in unique(avnn$SITE)){
  train <-  avnn[avnn$SITE!=i,]
  test <-  avnn[avnn$SITE==i,]
  #lm.1 <- glm(as.numeric(ControlType == "case")~ .- SITE - StudySubjectID - CaseID, 
  #          data = train, family = "binomial")
  
  lm.1 <- glm(as.numeric(ControlType == "case")~ AlteredMentalStatus + FocalNeuroFindings + SubInj_Head + SubInj_TorsoTrunk + 
                Predisposed + HighriskDiving + HighriskHitByCar + HighriskMVC + 
                HighriskOtherMV + AxialLoadAnyDoc + axialloadtop + FocalNeuroFindings2 + 
                PainNeck2 + Torticollis2 + subinj_Head2 + subinj_Face2, 
            data = train, family = "binomial")
  num_preds[avnn$SITE==i] <-  predict(lm.1, type = "response", newdata=test)
}
class_preds <- as.numeric(num_preds > 0.079)
rocs(avnn$ControlType == "case", num_preds, class_preds,  avnn_reprod, avnn_reprod2)
LR_metrics <- metrics(avnn$ControlType == "case", class_preds)
```


```{r, echo = FALSE, include=TRUE, message = FALSE, warning= FALSE, out.width = '80%', fig.align='center'}
kable(round(LR_metrics,2), align = 'cccc',
      caption = 'Logistic Regression') %>%
kable_styling(bootstrap_options = "striped", full_width = F, latex_options = "HOLD_position")
```

To interpret potential patterns in the errors, we examine the means of each feature for the misclassified groups and correctly classified groups from the 
regression tree model, as this model resulted in the best performance compared to the baseline. In the bar graphs 
below, we analyze the average value of each feature in the set the model correclty labeled, versus the 
set on data points the model miss-classified for both true positives and true negatives. Although the decision tree split the data on 4 features,
 the average values for all of the features are presented below, to help 
 identify any patterns that the model may have missed. For example, more cases 
 where the "ambulatory" feature was present in the data were misclassified: both in
  the true positive and true negative case. However, the top node of the decision
  tree, FocalNeuroFindings2, is present in a high percentage of the correctly labelled positive cases,
   while it is present in a smaller percentage of the incorreclty labelled negative cases. Ultimately, 
   these bar plots demonstrate the trade-off between sensitivity and specificity. If we were to 
   decrease the incorrectly labelled false positives, we would increase the number of 
   false negatives.

```{r, echo = FALSE, include=TRUE, message = FALSE, warning= FALSE, out.width = '80%', fig.align='center', fig.height = 5, fig.width = 11}
eval_correct <- avnn[avnn$ControlType == "case" & class_preds_tree == 1, ][, 5:36]

eval_false <- avnn[avnn$ControlType == "case" & class_preds_tree == 0, ][, 5:36]

means_correct <- apply(eval_correct, MARGIN = 2, function(x){
  col_num <- as.numeric(x)
  return(mean(col_num))
})

means_false <- apply(eval_false, MARGIN = 2, function(x){
  col_num <- as.numeric(x)
  return(mean(col_num))
})

correct <- data.frame(Average = means_correct, group=rep("True Positive"), Feature = names(means_correct))
false <- data.frame(Average = means_false, group=rep("False Negative"), Feature = names(means_false))

to_plot <- rbind(correct, false)
to_plot$group <- factor(to_plot$group)

p1 <- ggplot(to_plot, aes(y = Feature)) + geom_bar(stat="identity", aes(x = Average, fill = group), position = "dodge") + labs(title = "Average Values: True Positives")  +
  scale_fill_manual(values = c("palegreen3", "palegreen4")) + theme_bw()

eval_correct <- avnn[avnn$ControlType != "case" & class_preds_tree == 0, ][, 5:36]

eval_false <- avnn[avnn$ControlType != "case" & class_preds_tree == 1, ][, 5:36]

means_correct <- apply(eval_correct, MARGIN = 2, function(x){
  col_num <- as.numeric(x)
  return(mean(col_num))
})

means_false <- apply(eval_false, MARGIN = 2, function(x){
  col_num <- as.numeric(x)
  return(mean(col_num))
})

correct <- data.frame(Average = means_correct, group=rep("True Negative"), Feature = names(means_correct))
false <- data.frame(Average = means_false, group=rep("False Positive"), Feature = names(means_false))

to_plot <- rbind(correct, false)
to_plot$group <- factor(to_plot$group)

p2 <- ggplot(to_plot, aes(y = Feature)) + geom_bar(stat="identity", aes(x = Average, fill = group), position = "dodge") + labs(title = "Average Values: True Negatives/False Negatives")  +
  scale_fill_manual(values = c("palegreen3", "palegreen4")) + theme_bw()

ggarrange(p1, p2)
```



# Summary
By investigating primary source data for cervical spine injury (CSI) in children, we derived and validated a clinical decision rule that aims to guide imaging decisions for children who experienced blunt trauma (e.g., altered mental status, focal neurologic deficits, complaint of neck pain, torticollis). Based on our results and stability check, we believe that our decision rule will (1) provide satisfactory performance when applied to future data and (2) perform better than the baseline case in terms of both specificity and sensitivity.

# Citations 

[1] Leonard, J. C., Browne, L. R., Ahmad, F. A., Schwartz, H., Wallendorf, M., Leonard, J. R., Lerner, E. B., & Kuppermann, N. (2019). Cervical Spine Injury Risk Factors in Children With Blunt Trauma. Pediatrics, 144(1), e20183221. https://doi.org/10.1542/peds.2018-3221

[2] Terry Therneau and Beth Atkinson (2019). rpart: Recursive Partitioning and Regression Trees. R package version 4.1-15. https://CRAN.R-project.org/package=rpart

