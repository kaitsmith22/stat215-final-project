---
title: "Learning and Evaluating Clinical Decision Rules for Cervical Spine Injury"
author: "Jaewon Saw, Jeffrey Cheng, Ahmed Eldeeb, and Kaitlin Smith"
date: 'December, 2022'
header-includes:
   - \usepackage{float}
   - \usepackage{booktabs}
output: 
  pdf_document:
    number_sections: true

---

```{r setup, echo = FALSE, message=FALSE, warning=FALSE}
# load in useful packages
library(tidyverse)
library(forcats)
library(lubridate)
library(stringr)
library(RColorBrewer)
library(ggpubr)
library("reshape")
library(formatR)
# R.utils::sourceDirectory("./R/", modifiedOnly = F, recursive = F) # useful functions

source("./routines.R")

# set default knitr chunks
knitr::opts_chunk$set(
  echo = FALSE,  # don't print the code chunk
  warning = FALSE,  # don't print warnings
  message = FALSE,  # don't print messages
  fig.width = 6,  # set default width of figures
  fig.height = 4,  # set default height of figures
  fig.align = "center",  # always align figure in center
  fig.pos = "H",  # always plot figure at the exact location of the code chunk
  cache = FALSE)  # don't cache results
```

# Introduction

The goal of this project is to create a clinical decision rule to identify 
children who are most likely to have a cervical spine injury (CSI). The adverse 
effects of immobilizing children and subjecting children to ionizing radiation 
motivates such a rule, as there is a desire to minimize the number of children 
unnecessarily subject to radiographic assessment while continuing to maintain 
high sensitivity.

# Data

## Data Collection
The data was taken from the Pediatric Emergency Care Applied Research Network 
(PECARN) public use dataset titled "Predicting Cervical Spine Injury (CSI) in 
Children: A Multi-Centered Case-Control Analysis". A total of 17 PECARN sites 
participated in the study, with a total of 3,314 subjects included in this 
dataset. This dataset was collected between January 2000 and December 2004 and 
was initially procured for the purpose of creating a decision rule for 
identifying factors associated with CSI. The results for this study are 
presented in "Factors Associated With Cervical Spine Injury in Children After
Blunt Trauma" by Leonard et al.

Of the 3,314 records, 540 are deemed positive cervical spine injuries from 
radiology reports or spine consultation. These positive injury records were 
verified by the principal investigator of Leonard et al. and by a pediatric 
neurosurgeon[CITATION]. The remaining 2,774 controls fall into three control 
groups: 1,060 unmatched random controls, 1,012 mechanism-of-injury and age 
matched controls, and 702 age-matched EMS controls. 

```{r EDA-Race}
av <- read.csv("./CSpine/CSV datasets/analysisvariables.csv")
radiology <- read.csv("./CSpine/CSV datasets/radiologyreview.csv")
injury <- read.csv("./CSpine/CSV datasets/injuryclassification.csv")
site <- read.csv("./CSpine/CSV datasets/clinicalpresentationsite.csv")
ems <- read.csv("./CSpine/CSV datasets/clinicalpresentationfield.csv")
outside <- read.csv("./CSpine/CSV datasets/clinicalpresentationoutside.csv")
dem <- read.csv("./CSpine/CSV datasets/demographics.csv")

names(dem)[names(dem) == "studysubjectid"] <- "StudySubjectID"
av_dem <- merge(av, dem, by="StudySubjectID")

plot_demographics_count <- ggplot(data = av_dem) + 
                            geom_count(mapping = aes(x = SITE, y = Race))+
                            ggtitle("Count of Patients by Site and Race")+
                            xlab("Site")+
                            guides(fill=guide_legend(title="Count"))+
                            scale_x_continuous(breaks=seq(1,17,1))
plot_demographics_count
```

In the figure above, it is clear that the patients are not equally distributed 
across PECARN sites, as the racial distribution of patients varies visually 
across sites.

Leonard et al. 8 major variables that are associated with CSI: altered mental 
status, focal neurological deficits, complaint of neck pain, torticollis,
substantial injury to the torso, predisposing condition, high-risk motor vehicle
crash, and diving. Our analyses focused on these factors, as these were the only 
clinical variables that were provided for all 3,314 records. Each variable is 
not directly comparable with each other, but the variables have already been 
one-hot encoded in the PECARN dataset.

## Cleaning
From the original PECARN public use dataset, the amount of cleaning depended on 
the classifier used. The baseline rule from Leonard et al. and decision trees 
were both tolerant of missing data, so no additional cleaning was done for these 
processes. For the lasso selection of variables and logistic regression, records 
with missing values were removed from the analysis. 

## Training and Evaluation Split
We assume that in practice, our decision rule will be deployed at hospitals not 
included in the dataset. Then, to simulate the performance of the models in 
practice, the data was split into test and training sets based on sites. 

Sites 5, 16, and 17 were randomly chosen as the evaluation sites. 
Leave-one-out-cross-validation (LOOCV) was conducted over the remaining sites 
during training. 

# Stability

## Model Perturbation
The effect of varying the elasticnet mixing parameter, or the mixture of the 
L1 lasso penalty and L2 ridge penalty, was analyzed. The top 15 variables 
selected by the specific mixing parameter were then used to perform logistic 
regression. To compare the models, the area under the ROC curves (AUC) were 
calculated.

```{r glmnet-alpha}
library(glmnet)
library(pROC)
av <- read.csv("./CSpine/CSV datasets/analysisvariables.csv")

vars <- colnames(av)[5:ncol(av)]

# remove testing data
data_train <- av %>% filter(SITE != 5 & SITE != 16 & SITE != 15)

y <- unlist(lapply(data_train$ControlType, function(x){
  if (x == "case")
    {return (1)} 
  else 0}))

data_train$y <- y


dat_no_na <- data_train[rowSums(is.na(data_train)) == 0, ]


# Setting alpha = 1 implements lasso regression
X <- dat_no_na[, vars]

# get baseline deicsion rule results 
avnn_reprod <-  as.numeric(dat_no_na$AlteredMentalStatus == 1 |  dat_no_na$FocalNeuroFindings == 1 | 
                            dat_no_na$PainNeck == 1 | dat_no_na$SubInj_TorsoTrunk == 1 | dat_no_na$Predisposed == 1 |
                            dat_no_na$HighriskDiving == 1 | dat_no_na$HighriskHitByCar == 1 | 
                            dat_no_na$HighriskMVC == 1 |   dat_no_na$axialloadtop == 1 | dat_no_na$Clotheslining == 1)

avnn_reprod2 <- as.numeric(dat_no_na$AlteredMentalStatus == 1 | dat_no_na$FocalNeuroFindings == 1 |  
                            dat_no_na$PainNeck == 1| dat_no_na$SubInj_TorsoTrunk == 1 | 
                            dat_no_na$HighriskDiving == 1 |dat_no_na$HighriskMVC == 1)

alpha_range = seq(1,0,-0.1)
sens_low = c()
sens_high = c()
sens = c()

spec_low = c()
spec_high = c()
spec = c()

auc_list = c()
for (j in alpha_range){
  X <- dat_no_na[, vars]
  # find value of lambda that minimizes the deviance
  lasso_reg <- glmnet::cv.glmnet(as.matrix(X),  y = dat_no_na$y, alpha = j, standardize = TRUE, nfolds = 14)
  
  lambda_best <- lasso_reg$lambda.min
  
  
  # get lasso model with optimal performance found with cross validation
  
  lasso_model <- glmnet(x = X, y = dat_no_na$y, alpha = j, standardize = TRUE)
  
  ord <- lasso_model$beta
  
  sum_0 <- apply(ord, MARGIN = 1, function(r){
    return(sum(r == 0))
  })
  
  # get features in the order they are added to the model
  sorted_sum <- data.frame(sort(sum_0))
  
  selected_vars <- rownames(sorted_sum)[1:16]
  
  # get cross validation results for the different sites
  
  num_preds <- rep(0, nrow(dat_no_na))
  class_preds <- rep(0, nrow(dat_no_na))
  
  for (i in unique(dat_no_na$SITE)){
    train <-  dat_no_na[dat_no_na$SITE!=i,]
    test <-  dat_no_na[dat_no_na$SITE==i,]
    
    X <- train[, selected_vars]
    X_test <- test[, selected_vars]
    
    lasso_model <- glmnet(x = X, y = train$y, alpha = j, lambda = lambda_best, standardize = TRUE)
    
    num_preds[dat_no_na$SITE==i] <-  predict(lasso_model, type = "response", newx = as.matrix(X_test))
    
  }
  
  class_preds <- as.numeric(num_preds > 0.085)
  
  metrics(dat_no_na$ControlType == "case", class_preds)



  class_preds <- as.numeric(num_preds > 0.085)
  results <- metrics(dat_no_na$ControlType == "case", class_preds)

  sens = c(sens, results[1,1])
  sens_low = c(sens_low, results[1,2])
  sens_high = c(sens_high, results[1,3])

  spec = c(spec, results[2,1])
  spec_low = c(spec_low, results[2,2])
  spec_high = c(spec_high, results[2,3])
  
  # rocs(dat_no_na$ControlType == "case", num_preds, class_preds,  avnn_reprod, avnn_reprod2)
  auc_list <- c(auc_list, auc(roc(as.numeric(dat_no_na$ControlType == "case"), num_preds, plot=F, lty=1, lwd=2)))
  
  
}
alpha_auc_df <- data.frame(x = alpha_range,
                                     y = auc_list)

ggplot(alpha_auc_df, aes(x, y)) + geom_point() + 
  xlab('Lasso-Ridge Mixing Parameter') + 
ylab('AUC') + ggtitle('Area under ROC Curve Across Varying Mixing Parameters') 
# CI_bootstrap_sens <-round(data.frame(x = alpha_range,
#                                      sens = sens,
#                                      sens_low = sens_low,
#                                      sens_high = sens_high), 4)
#   
# # Creating scatter plot with its
# # confindence intervals
# ggplot(CI_bootstrap_sens, aes(x, sens)) + geom_point() + 
# geom_errorbar(aes(ymin = sens_low, ymax = sens_high))+ xlab('Lasso-Ridge Mixing Parameter') + 
# ylab('Sensitivity Estimate') + ggtitle('Area under ROC Curve Across Varying Mixing Parameters')
# 
# CI_bootstrap_spec <-round(data.frame(x = alpha_range,
#                                      spec = spec,
#                                      spec_low = spec_low,
#                                      spec_high = spec_high), 4)
#   
# # Creating scatter plot with its
# # confindence intervals
# ggplot(CI_bootstrap_spec, aes(x, spec)) + geom_point() + 
# geom_errorbar(aes(ymin = spec_low, ymax = spec_high)) + xlab('Lasso-Ridge Mixing Parameter') +
# ylab('Specificity Estimate') + ggtitle('Specificity Estimates Across Varying Mixing Parameters')

```
The nonzero $\alpha$ parameters yield similar confidence intervals for both 
sensitivity and specificity, but the pure ridge regression $\alpha$ yields 
significantly worse performance.

As for introducing a perturbation to logistic regression classification, one can 
adjust the classification threshold. The effect of this is captured in the ROC 
curve of the model presented in the previous section.


## Data Perturbation

```{r Base-model}
avn <- av[av$SITE != 15 & av$SITE != 16 & av$SITE != 7,]
vars <- colnames(av)[5:26]
avnn<- avn %>% drop_na(vars)

num_preds <- rep(0, nrow(avnn))
class_preds <- rep(0, nrow(avnn))


#  [1] "AlteredMentalStatus" "FocalNeuroFindings"  "HighriskDiving"      "HighriskHitByCar"
#  [5] "HighriskMVC"         "PainNeck2"           "Predisposed"         "SubInj_TorsoTrunk"
#  [9] "FocalNeuroFindings2" "axialloadtop"        "Torticollis2"        "LOC"
# [13] "HighriskOtherMV"     "Clotheslining"       "SubInj_Head"         "AxialLoadAnyDoc"


#  [1] "AlteredMentalStatus" "FocalNeuroFindings"  "SubInj_Head"         "SubInj_TorsoTrunk"  
#  [5] "Predisposed"         "HighriskDiving"      "HighriskHitByCar"    "HighriskMVC"        
#  [9] "HighriskOtherMV"     "AxialLoadAnyDoc"     "axialloadtop"        "FocalNeuroFindings2"
# [13] "PainNeck2"           "Torticollis2"        "subinj_Head2"        "subinj_Face2"  


for (i in unique(avnn$SITE)){
  train <-  avnn[avnn$SITE!=i,]
  test <-  avnn[avnn$SITE==i,]

  #lm.1 <- glm(as.numeric(ControlType == "case")~ .- SITE - StudySubjectID - CaseID, 
  #          data = train, family = "binomial")
  
  lm.1 <- glm(as.numeric(ControlType == "case")~ AlteredMentalStatus + FocalNeuroFindings + HighriskHitByCar +
                HighriskMVC + PainNeck2 + Predisposed + SubInj_TorsoTrunk + FocalNeuroFindings2 +
                axialloadtop + Torticollis2 + LOC + HighriskOtherMV + Clotheslining + 
                subinj_Face2 + AxialLoadAnyDoc, 
            data = train, family = "binomial")
}
class_preds <- as.numeric(num_preds > 0.079)

base_metrics <- metrics(avnn$ControlType == "case", class_preds)
```


### Change in Covariate Distribution During Testing
To examine the effect of changing the covariate distribution of the test set, 
the model was evaluated on the held-out site 7. As seen in Figure 1, site 7 has 
a higher proportion of of subjects of race group ND.

```{r Covariate}
# data from site 7
av7 <- av[av$SITE == 7,]
vars <- colnames(av)[5:26]
av7n<- av7 %>% drop_na(vars)

# apply base model
num_preds <- rep(0, nrow(av7n))
class_preds <- rep(0, nrow(av7n))

num_preds <-  predict(lm.1, type = "response", newdata=av7n)


class_preds <- as.numeric(num_preds > 0.079)
# rocs(av7n$ControlType == "case", num_preds, class_preds,  avnn_reprod, avnn_reprod2)
site7_metrics <- metrics(av7n$ControlType == "case", class_preds)
```
As expected, worse performance is observed, as the records from site 7 were 
excluded from training. 

### Stability under Subsampling
The model was evaluated on just the positive injury records and the EMS control 
group.

```{r Subsampling}
# case and ems data
avn_ce <- av[av$ControlType == 'case' | av$ControlType == 'ems',]
vars <- colnames(av)[5:26]
avnn_ce<- avn_ce %>% drop_na(vars)

# apply base model
num_preds <- rep(0, nrow(avnn_ce))
class_preds <- rep(0, nrow(avnn_ce))

num_preds <-  predict(lm.1, type = "response", newdata=avnn_ce)


class_preds <- as.numeric(num_preds > 0.079)
# rocs(avnn_ce$ControlType == "case", num_preds, class_preds,  avnn_reprod, avnn_reprod2)
subsample_metrics <- metrics(avn_ce$ControlType == "case", class_preds)
```



### Stability under Bootstrapping
In addition, we observed the stability our model under bootstraps. For the lasso 
selection of variables, we analyze the frequency of the original 15 factors in 
our model in the top 15 factors selected by lasso generated by the bootstrapped 
data.



```{r Bootstrap-Lasso}
source("./routines.R")
library(glmnet)
library(ggplot2)

av <- read.csv("./CSpine/CSV datasets/analysisvariables.csv")

vars <- colnames(av)[5:ncol(av)]

# remove testing data
data_train <- av %>% filter(SITE != 7 & SITE != 16 & SITE != 15)

y <- unlist(lapply(data_train$ControlType, function(x){
  if (x == "case")
    {return (1)} 
  else 0}))

data_train$y <- y


dat_no_na <- data_train[rowSums(is.na(data_train)) == 0, ]

# get baseline deicsion rule results 
avnn_reprod <-  as.numeric(dat_no_na$AlteredMentalStatus == 1 |  dat_no_na$FocalNeuroFindings == 1 | 
                            dat_no_na$PainNeck == 1 | dat_no_na$SubInj_TorsoTrunk == 1 | dat_no_na$Predisposed == 1 |
                            dat_no_na$HighriskDiving == 1 | dat_no_na$HighriskHitByCar == 1 | 
                            dat_no_na$HighriskMVC == 1 |   dat_no_na$axialloadtop == 1 | dat_no_na$Clotheslining == 1)

avnn_reprod2 <- as.numeric(dat_no_na$AlteredMentalStatus == 1 | dat_no_na$FocalNeuroFindings == 1 |  
                            dat_no_na$PainNeck == 1| dat_no_na$SubInj_TorsoTrunk == 1 | 
                            dat_no_na$HighriskDiving == 1 |dat_no_na$HighriskMVC == 1)

# Setting alpha = 1 implements lasso regression
X <- dat_no_na[, vars]

# find value of lambda that minimizes the deviance
lasso_reg <- glmnet::cv.glmnet(as.matrix(X),  y = dat_no_na$y, alpha = 1, standardize = TRUE, nfolds = 14)

lambda_best <- lasso_reg$lambda.min

# get lasso model with optimal performance found with cross validation

lasso_model <- glmnet(x = X, y = dat_no_na$y, alpha = 1, standardize = TRUE)

ord <- lasso_model$beta

sum_0 <- apply(ord, MARGIN = 1, function(r){
  return(sum(r == 0))
})

# get features in the order they are added to the model
sorted_sum <- data.frame(sort(sum_0))

selected_names <- rownames(sorted_sum)[1:15]



selected_names_counts = integer(length(selected_names))

sens_low = c()
sens_high = c()
sens = c()

spec_low = c()
spec_high = c()
spec = c()

# do n_bootstraps bootstraps (sample with replacement)
set.seed(1)
n_bootstraps = 100
for (i in 1:n_bootstraps){
  # resample from data_train
  data_train_boot <- data_train[sample(nrow(data_train),size=nrow(data_train),replace=TRUE),]
  y <- unlist(lapply(data_train_boot$ControlType, function(x){
  if (x == "case")
    {return (1)} 
  else 0}))

  data_train_boot$y <- y
  
  
  dat_no_na <- data_train_boot[rowSums(is.na(data_train_boot)) == 0, ]
  X <- dat_no_na[, vars]

  # find value of lambda that minimizes the deviance
  lasso_reg <- glmnet::cv.glmnet(as.matrix(X),  y = dat_no_na$y, alpha = 1, standardize = TRUE, nfolds = 14)
  
  lambda_best <- lasso_reg$lambda.min
  
  # get lasso model with optimal performance found with cross validation
  
  lasso_model <- glmnet(x = X, y = dat_no_na$y, alpha = 1, standardize = TRUE)
  
  ord <- lasso_model$beta
  
  sum_0 <- apply(ord, MARGIN = 1, function(r){
    return(sum(r == 0))
  })
  
  # get features in the order they are added to the model
  sorted_sum <- data.frame(sort(sum_0))
  
  bootstrapped_names <- rownames(sorted_sum)[1:16]
   
  for (j in 1:length(selected_names)){
    if (selected_names[j] %in% bootstrapped_names){
      selected_names_counts[j] <- selected_names_counts[j] + 1
    }
  }
  selected_names_freq <- selected_names_counts/n_bootstraps
  
  
  # num_preds <- rep(0, nrow(dat_no_na))
  # class_preds <- rep(0, nrow(dat_no_na))
  # 
  # for (i in unique(dat_no_na$SITE)){
  #   train <-  dat_no_na[dat_no_na$SITE!=i,]
  #   test <-  dat_no_na[dat_no_na$SITE==i,]
  #   
  #   X <- train[, vars]
  #   X_test <- test[, vars]
  #   
  #   lasso_model <- glmnet(x = X, y = train$y, alpha = 1, lambda = lambda_best, standardize = TRUE)
  #   
  #   num_preds[dat_no_na$SITE==i] <-  predict(lasso_model, type = "response", newx = as.matrix(X_test))
  #   
  # }
  # 
  # 
  # class_preds <- as.numeric(num_preds > 0.085)
  # results <- metrics(dat_no_na$ControlType == "case", class_preds)
  # 
  # sens = c(sens, results[1,1])
  # sens_low = c(sens_low, results[1,2])
  # sens_high = c(sens_high, results[1,3])
  # 
  # spec = c(spec, results[2,1])
  # spec_low = c(spec_low, results[2,2])
  # spec_high = c(spec_high, results[2,3])
  

  
}
names(selected_names_freq) <- selected_names

barplot(selected_names_freq, 
        main = "Frequency of Selected Features During Bootstrapping", 
        xlab = 'Feature', ylab = 'Frequency', las = 2, cex.names = 0.5)
```

We can see that the top of the original factors appear in the bootstrapped 
data's top factors very frequently, whereas the lower of the original factors 
appear less frequently.

The stability of the logistic regression classifier under bootstrapping was also 
analyzed. The sensitivity and specificity confidence intervals of each bootstrap 
were recorded and are plotted below.
```{r Bootstrap-Logistic}
for (j in 1:n_bootstraps){
  avn_boot <- avn[sample(nrow(avn),size=nrow(avn),replace=TRUE),]
  avnn <- avn_boot %>% drop_na(vars)
  
  num_preds <- rep(0, nrow(avnn))
  class_preds <- rep(0, nrow(avnn))
  
  
  #  [1] "AlteredMentalStatus" "FocalNeuroFindings"  "HighriskDiving"      "HighriskHitByCar"
  #  [5] "HighriskMVC"         "PainNeck2"           "Predisposed"         "SubInj_TorsoTrunk"
  #  [9] "FocalNeuroFindings2" "axialloadtop"        "Torticollis2"        "LOC"
  # [13] "HighriskOtherMV"     "Clotheslining"       "SubInj_Head"         "AxialLoadAnyDoc"
  
  
  #  [1] "AlteredMentalStatus" "FocalNeuroFindings"  "SubInj_Head"         "SubInj_TorsoTrunk"  
  #  [5] "Predisposed"         "HighriskDiving"      "HighriskHitByCar"    "HighriskMVC"        
  #  [9] "HighriskOtherMV"     "AxialLoadAnyDoc"     "axialloadtop"        "FocalNeuroFindings2"
  # [13] "PainNeck2"           "Torticollis2"        "subinj_Head2"        "subinj_Face2"  
  
  
  for (i in unique(avnn$SITE)){
    train <-  avnn[avnn$SITE!=i,]
    test <-  avnn[avnn$SITE==i,]
  
    #lm.1 <- glm(as.numeric(ControlType == "case")~ .- SITE - StudySubjectID - CaseID, 
    #          data = train, family = "binomial")
    
    lm.1 <- glm(as.numeric(ControlType == "case")~ AlteredMentalStatus + FocalNeuroFindings + HighriskHitByCar +
                  HighriskMVC + PainNeck2 + Predisposed + SubInj_TorsoTrunk + FocalNeuroFindings2 +
                  axialloadtop + Torticollis2 + LOC + HighriskOtherMV + Clotheslining + 
                  subinj_Face2 + AxialLoadAnyDoc, 
              data = train, family = "binomial")
  
    num_preds[avnn$SITE==i] <-  predict(lm.1, type = "response", newdata=test)
  }
  
  class_preds <- as.numeric(num_preds > 0.079)
  
  results <- metrics(avnn$ControlType == "case", class_preds)

  sens = c(sens, results[1,1])
  sens_low = c(sens_low, results[1,2])
  sens_high = c(sens_high, results[1,3])

  spec = c(spec, results[2,1])
  spec_low = c(spec_low, results[2,2])
  spec_high = c(spec_high, results[2,3])
}

CI_bootstrap_sens <-round(data.frame(x = 1:n_bootstraps,
                                     sens = sens,
                                     sens_low = sens_low,
                                     sens_high = sens_high), 4)
  
# Creating scatter plot with its
# confindence intervals
ggplot(CI_bootstrap_sens, aes(x, sens)) + geom_point() + 
geom_errorbar(aes(ymin = sens_low, ymax = sens_high))+ xlab('Sample') + 
ylab('Sensitivity Estimate') + ggtitle('Sensitivity Estimates Across Bootstraps')

CI_bootstrap_spec <-round(data.frame(x = 1:n_bootstraps,
                                     spec = spec,
                                     spec_low = spec_low,
                                     spec_high = spec_high), 4)
  
# Creating scatter plot with its
# confindence intervals
ggplot(CI_bootstrap_spec, aes(x, spec)) + geom_point() + 
geom_errorbar(aes(ymin = spec_low, ymax = spec_high)) + xlab('Sample') +
ylab('Specificity Estimate') + ggtitle('Specificity Estimates Across Bootstraps')

```
The sensitivity estimates appear to be more stable than the specificity 
estimates, as the sensitivity confidence intervals share more overlap.


